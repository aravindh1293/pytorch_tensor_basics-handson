{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image Classification on CIFAR-10 with nn.Sequential in PYTORCH:.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPYFucwVkmGONWe+/bLa30B",
      "include_colab_link": False
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2ce905cebff74936ab7d3721802cd51c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0b92827f538d491a93f0a938f164ebee",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c475ccbc60be4383b1805d92c7aab345",
              "IPY_MODEL_12d8b501a7854c0c9f548ca251260186"
            ]
          }
        },
        "0b92827f538d491a93f0a938f164ebee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c475ccbc60be4383b1805d92c7aab345": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_04be55e685e4474885703c022bb42dac",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3af3b54fa1234193b1a6b5f420450417"
          }
        },
        "12d8b501a7854c0c9f548ca251260186": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7542ff28fd954685b1a8ed5575bfcbd3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "170500096it [00:02, 80808402.95it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0eb56b6a207a4fda9ff21826f428f879"
          }
        },
        "04be55e685e4474885703c022bb42dac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3af3b54fa1234193b1a6b5f420450417": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7542ff28fd954685b1a8ed5575bfcbd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0eb56b6a207a4fda9ff21826f428f879": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aravindh1293/pytorch_tensor_basics-handson/blob/master/nn_Sequential_in_PYTORCH_Image_Classification_on_CIFAR_10_with.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qBdfz5lDDZG",
        "colab_type": "text"
      },
      "source": [
        "# Image Classification on CIFAR-10 with nn.Sequential in PYTORCH:\n",
        "\n",
        "* Download data.\n",
        "* load data.\n",
        "* dataloader.\n",
        "* model arch.\n",
        "* train loop.\n",
        "* eval loop.\n",
        "* train vs val loss plot.\n",
        "* transfer learning.\n",
        "* tweak best model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcPMc4Y7F_M0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "outputId": "75a174bd-d908-4549-ef86-7aa2fc6df169"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Feb 24 14:14:33 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.48.02    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   55C    P8    10W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNZoLki8jIRm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cs48c8p8leXz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "aug  = transforms.Compose([\n",
        "                           transforms.ToTensor(),\n",
        "                           transforms.Normalize([0.5,0.5,0.5], [0.5,0.5,0.5])\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2bSSdoojsvY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123,
          "referenced_widgets": [
            "2ce905cebff74936ab7d3721802cd51c",
            "0b92827f538d491a93f0a938f164ebee",
            "c475ccbc60be4383b1805d92c7aab345",
            "12d8b501a7854c0c9f548ca251260186",
            "04be55e685e4474885703c022bb42dac",
            "3af3b54fa1234193b1a6b5f420450417",
            "7542ff28fd954685b1a8ed5575bfcbd3",
            "0eb56b6a207a4fda9ff21826f428f879"
          ]
        },
        "outputId": "7dac833c-cdad-4984-f1af-f00a5cd7936a"
      },
      "source": [
        "traindata = datasets.CIFAR10(root=\"./data\", train=True,transform=aug, download=True)\n",
        "testdata = datasets.CIFAR10(root=\"./data\", train=False,  transform=aug, download=True)\n",
        "\n",
        "trainloader = DataLoader(dataset=traindata, batch_size=64, num_workers=4, shuffle=True)\n",
        "testloader = DataLoader(dataset=testdata, batch_size=64, num_workers=4, shuffle=True)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2ce905cebff74936ab7d3721802cd51c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMhvCcBklbKU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Conv2d(3, 6, 3),\n",
        "    nn.MaxPool2d(2),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(6, 12, 3),\n",
        "    nn.MaxPool2d(2),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(12, 18, 3),\n",
        "    nn.MaxPool2d(2),\n",
        "    nn.ReLU(),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(72, 10)\n",
        ")\n",
        "\n",
        "opt = torch.optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnal5a1knwjJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "985ecde7-ac9b-49eb-a09f-8a447951b5bb"
      },
      "source": [
        "#train\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = model.to(device)\n",
        "loss_acc = 0.0\n",
        "for epoch in range(10):\n",
        "    for img, label in trainloader:\n",
        "        img = img.to(device)\n",
        "        label = label.to(device)\n",
        "        opt.zero_grad()\n",
        "        pred = model(img)\n",
        "        loss = criterion(pred, label)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        loss_acc+=loss.item()\n",
        "    print(f\"loss after epoch {epoch} : {loss_acc/2000}\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss after epoch 0 : 0.41429054230451584\n",
            "loss after epoch 1 : 0.826247534930706\n",
            "loss after epoch 2 : 1.2383938112556934\n",
            "loss after epoch 3 : 1.649117728382349\n",
            "loss after epoch 4 : 2.058698485761881\n",
            "loss after epoch 5 : 2.4676025170981886\n",
            "loss after epoch 6 : 2.8762028035521507\n",
            "loss after epoch 7 : 3.283917964488268\n",
            "loss after epoch 8 : 3.6906035619676114\n",
            "loss after epoch 9 : 4.096104060679674\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KT1uLoPoAMf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training\n",
        "def train(epoch):\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        opt.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        print(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "            % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "\n",
        "def test():\n",
        "    global best_acc\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "            print(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "                % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoViIejG0hXE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "38dfa7d6-c956-4cf1-8a00-df091437568f"
      },
      "source": [
        "train(10)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 10\n",
            "0 782 Loss: 0.735 | Acc: 75.000% (48/64)\n",
            "1 782 Loss: 0.862 | Acc: 68.750% (88/128)\n",
            "2 782 Loss: 0.903 | Acc: 68.750% (132/192)\n",
            "3 782 Loss: 0.899 | Acc: 68.359% (175/256)\n",
            "4 782 Loss: 0.931 | Acc: 66.875% (214/320)\n",
            "5 782 Loss: 0.914 | Acc: 67.448% (259/384)\n",
            "6 782 Loss: 0.933 | Acc: 67.411% (302/448)\n",
            "7 782 Loss: 0.933 | Acc: 67.383% (345/512)\n",
            "8 782 Loss: 0.924 | Acc: 67.708% (390/576)\n",
            "9 782 Loss: 0.911 | Acc: 68.125% (436/640)\n",
            "10 782 Loss: 0.933 | Acc: 67.756% (477/704)\n",
            "11 782 Loss: 0.928 | Acc: 67.839% (521/768)\n",
            "12 782 Loss: 0.946 | Acc: 67.308% (560/832)\n",
            "13 782 Loss: 0.951 | Acc: 66.741% (598/896)\n",
            "14 782 Loss: 0.951 | Acc: 66.875% (642/960)\n",
            "15 782 Loss: 0.955 | Acc: 66.602% (682/1024)\n",
            "16 782 Loss: 0.955 | Acc: 66.544% (724/1088)\n",
            "17 782 Loss: 0.956 | Acc: 66.927% (771/1152)\n",
            "18 782 Loss: 0.959 | Acc: 66.941% (814/1216)\n",
            "19 782 Loss: 0.973 | Acc: 66.406% (850/1280)\n",
            "20 782 Loss: 0.985 | Acc: 65.848% (885/1344)\n",
            "21 782 Loss: 0.991 | Acc: 65.696% (925/1408)\n",
            "22 782 Loss: 0.998 | Acc: 65.693% (967/1472)\n",
            "23 782 Loss: 0.984 | Acc: 66.211% (1017/1536)\n",
            "24 782 Loss: 0.985 | Acc: 65.875% (1054/1600)\n",
            "25 782 Loss: 0.988 | Acc: 65.685% (1093/1664)\n",
            "26 782 Loss: 0.998 | Acc: 65.509% (1132/1728)\n",
            "27 782 Loss: 0.990 | Acc: 65.848% (1180/1792)\n",
            "28 782 Loss: 0.987 | Acc: 65.733% (1220/1856)\n",
            "29 782 Loss: 0.992 | Acc: 65.729% (1262/1920)\n",
            "30 782 Loss: 0.988 | Acc: 65.827% (1306/1984)\n",
            "31 782 Loss: 0.986 | Acc: 65.967% (1351/2048)\n",
            "32 782 Loss: 0.984 | Acc: 65.862% (1391/2112)\n",
            "33 782 Loss: 0.991 | Acc: 65.763% (1431/2176)\n",
            "34 782 Loss: 0.990 | Acc: 65.714% (1472/2240)\n",
            "35 782 Loss: 0.994 | Acc: 65.582% (1511/2304)\n",
            "36 782 Loss: 0.990 | Acc: 65.709% (1556/2368)\n",
            "37 782 Loss: 0.994 | Acc: 65.584% (1595/2432)\n",
            "38 782 Loss: 0.991 | Acc: 65.745% (1641/2496)\n",
            "39 782 Loss: 0.997 | Acc: 65.703% (1682/2560)\n",
            "40 782 Loss: 1.004 | Acc: 65.663% (1723/2624)\n",
            "41 782 Loss: 1.003 | Acc: 65.551% (1762/2688)\n",
            "42 782 Loss: 1.003 | Acc: 65.552% (1804/2752)\n",
            "43 782 Loss: 1.009 | Acc: 65.341% (1840/2816)\n",
            "44 782 Loss: 1.004 | Acc: 65.451% (1885/2880)\n",
            "45 782 Loss: 1.008 | Acc: 65.387% (1925/2944)\n",
            "46 782 Loss: 1.011 | Acc: 65.426% (1968/3008)\n",
            "47 782 Loss: 1.003 | Acc: 65.690% (2018/3072)\n",
            "48 782 Loss: 1.005 | Acc: 65.561% (2056/3136)\n",
            "49 782 Loss: 1.007 | Acc: 65.438% (2094/3200)\n",
            "50 782 Loss: 1.006 | Acc: 65.319% (2132/3264)\n",
            "51 782 Loss: 1.011 | Acc: 65.415% (2177/3328)\n",
            "52 782 Loss: 1.006 | Acc: 65.566% (2224/3392)\n",
            "53 782 Loss: 1.005 | Acc: 65.596% (2267/3456)\n",
            "54 782 Loss: 1.003 | Acc: 65.767% (2315/3520)\n",
            "55 782 Loss: 1.003 | Acc: 65.765% (2357/3584)\n",
            "56 782 Loss: 1.003 | Acc: 65.844% (2402/3648)\n",
            "57 782 Loss: 1.006 | Acc: 65.652% (2437/3712)\n",
            "58 782 Loss: 1.007 | Acc: 65.572% (2476/3776)\n",
            "59 782 Loss: 1.007 | Acc: 65.443% (2513/3840)\n",
            "60 782 Loss: 1.009 | Acc: 65.369% (2552/3904)\n",
            "61 782 Loss: 1.010 | Acc: 65.247% (2589/3968)\n",
            "62 782 Loss: 1.009 | Acc: 65.278% (2632/4032)\n",
            "63 782 Loss: 1.012 | Acc: 65.186% (2670/4096)\n",
            "64 782 Loss: 1.013 | Acc: 65.168% (2711/4160)\n",
            "65 782 Loss: 1.014 | Acc: 65.080% (2749/4224)\n",
            "66 782 Loss: 1.014 | Acc: 65.065% (2790/4288)\n",
            "67 782 Loss: 1.016 | Acc: 64.982% (2828/4352)\n",
            "68 782 Loss: 1.015 | Acc: 64.946% (2868/4416)\n",
            "69 782 Loss: 1.019 | Acc: 64.978% (2911/4480)\n",
            "70 782 Loss: 1.018 | Acc: 64.987% (2953/4544)\n",
            "71 782 Loss: 1.019 | Acc: 65.017% (2996/4608)\n",
            "72 782 Loss: 1.023 | Acc: 64.983% (3036/4672)\n",
            "73 782 Loss: 1.023 | Acc: 64.844% (3071/4736)\n",
            "74 782 Loss: 1.022 | Acc: 64.833% (3112/4800)\n",
            "75 782 Loss: 1.025 | Acc: 64.679% (3146/4864)\n",
            "76 782 Loss: 1.027 | Acc: 64.590% (3183/4928)\n",
            "77 782 Loss: 1.027 | Acc: 64.503% (3220/4992)\n",
            "78 782 Loss: 1.027 | Acc: 64.517% (3262/5056)\n",
            "79 782 Loss: 1.027 | Acc: 64.453% (3300/5120)\n",
            "80 782 Loss: 1.026 | Acc: 64.448% (3341/5184)\n",
            "81 782 Loss: 1.029 | Acc: 64.386% (3379/5248)\n",
            "82 782 Loss: 1.033 | Acc: 64.194% (3410/5312)\n",
            "83 782 Loss: 1.029 | Acc: 64.323% (3458/5376)\n",
            "84 782 Loss: 1.031 | Acc: 64.301% (3498/5440)\n",
            "85 782 Loss: 1.029 | Acc: 64.317% (3540/5504)\n",
            "86 782 Loss: 1.030 | Acc: 64.242% (3577/5568)\n",
            "87 782 Loss: 1.030 | Acc: 64.258% (3619/5632)\n",
            "88 782 Loss: 1.030 | Acc: 64.238% (3659/5696)\n",
            "89 782 Loss: 1.032 | Acc: 64.253% (3701/5760)\n",
            "90 782 Loss: 1.029 | Acc: 64.320% (3746/5824)\n",
            "91 782 Loss: 1.030 | Acc: 64.266% (3784/5888)\n",
            "92 782 Loss: 1.031 | Acc: 64.298% (3827/5952)\n",
            "93 782 Loss: 1.030 | Acc: 64.279% (3867/6016)\n",
            "94 782 Loss: 1.029 | Acc: 64.326% (3911/6080)\n",
            "95 782 Loss: 1.026 | Acc: 64.421% (3958/6144)\n",
            "96 782 Loss: 1.024 | Acc: 64.514% (4005/6208)\n",
            "97 782 Loss: 1.028 | Acc: 64.525% (4047/6272)\n",
            "98 782 Loss: 1.025 | Acc: 64.552% (4090/6336)\n",
            "99 782 Loss: 1.023 | Acc: 64.641% (4137/6400)\n",
            "100 782 Loss: 1.023 | Acc: 64.558% (4173/6464)\n",
            "101 782 Loss: 1.025 | Acc: 64.491% (4210/6528)\n",
            "102 782 Loss: 1.025 | Acc: 64.487% (4251/6592)\n",
            "103 782 Loss: 1.026 | Acc: 64.513% (4294/6656)\n",
            "104 782 Loss: 1.026 | Acc: 64.524% (4336/6720)\n",
            "105 782 Loss: 1.027 | Acc: 64.460% (4373/6784)\n",
            "106 782 Loss: 1.027 | Acc: 64.398% (4410/6848)\n",
            "107 782 Loss: 1.027 | Acc: 64.395% (4451/6912)\n",
            "108 782 Loss: 1.027 | Acc: 64.407% (4493/6976)\n",
            "109 782 Loss: 1.027 | Acc: 64.389% (4533/7040)\n",
            "110 782 Loss: 1.027 | Acc: 64.344% (4571/7104)\n",
            "111 782 Loss: 1.027 | Acc: 64.314% (4610/7168)\n",
            "112 782 Loss: 1.028 | Acc: 64.228% (4645/7232)\n",
            "113 782 Loss: 1.028 | Acc: 64.241% (4687/7296)\n",
            "114 782 Loss: 1.027 | Acc: 64.266% (4730/7360)\n",
            "115 782 Loss: 1.026 | Acc: 64.251% (4770/7424)\n",
            "116 782 Loss: 1.027 | Acc: 64.263% (4812/7488)\n",
            "117 782 Loss: 1.027 | Acc: 64.235% (4851/7552)\n",
            "118 782 Loss: 1.028 | Acc: 64.194% (4889/7616)\n",
            "119 782 Loss: 1.029 | Acc: 64.219% (4932/7680)\n",
            "120 782 Loss: 1.028 | Acc: 64.282% (4978/7744)\n",
            "121 782 Loss: 1.028 | Acc: 64.203% (5013/7808)\n",
            "122 782 Loss: 1.029 | Acc: 64.190% (5053/7872)\n",
            "123 782 Loss: 1.030 | Acc: 64.189% (5094/7936)\n",
            "124 782 Loss: 1.029 | Acc: 64.175% (5134/8000)\n",
            "125 782 Loss: 1.030 | Acc: 64.174% (5175/8064)\n",
            "126 782 Loss: 1.030 | Acc: 64.210% (5219/8128)\n",
            "127 782 Loss: 1.030 | Acc: 64.233% (5262/8192)\n",
            "128 782 Loss: 1.028 | Acc: 64.256% (5305/8256)\n",
            "129 782 Loss: 1.029 | Acc: 64.219% (5343/8320)\n",
            "130 782 Loss: 1.029 | Acc: 64.229% (5385/8384)\n",
            "131 782 Loss: 1.029 | Acc: 64.216% (5425/8448)\n",
            "132 782 Loss: 1.030 | Acc: 64.156% (5461/8512)\n",
            "133 782 Loss: 1.031 | Acc: 64.109% (5498/8576)\n",
            "134 782 Loss: 1.030 | Acc: 64.109% (5539/8640)\n",
            "135 782 Loss: 1.028 | Acc: 64.212% (5589/8704)\n",
            "136 782 Loss: 1.028 | Acc: 64.234% (5632/8768)\n",
            "137 782 Loss: 1.028 | Acc: 64.198% (5670/8832)\n",
            "138 782 Loss: 1.026 | Acc: 64.276% (5718/8896)\n",
            "139 782 Loss: 1.026 | Acc: 64.241% (5756/8960)\n",
            "140 782 Loss: 1.027 | Acc: 64.207% (5794/9024)\n",
            "141 782 Loss: 1.028 | Acc: 64.140% (5829/9088)\n",
            "142 782 Loss: 1.027 | Acc: 64.194% (5875/9152)\n",
            "143 782 Loss: 1.026 | Acc: 64.214% (5918/9216)\n",
            "144 782 Loss: 1.026 | Acc: 64.224% (5960/9280)\n",
            "145 782 Loss: 1.026 | Acc: 64.191% (5998/9344)\n",
            "146 782 Loss: 1.026 | Acc: 64.190% (6039/9408)\n",
            "147 782 Loss: 1.027 | Acc: 64.126% (6074/9472)\n",
            "148 782 Loss: 1.027 | Acc: 64.104% (6113/9536)\n",
            "149 782 Loss: 1.027 | Acc: 64.104% (6154/9600)\n",
            "150 782 Loss: 1.027 | Acc: 64.125% (6197/9664)\n",
            "151 782 Loss: 1.028 | Acc: 64.062% (6232/9728)\n",
            "152 782 Loss: 1.026 | Acc: 64.103% (6277/9792)\n",
            "153 782 Loss: 1.027 | Acc: 64.073% (6315/9856)\n",
            "154 782 Loss: 1.027 | Acc: 64.093% (6358/9920)\n",
            "155 782 Loss: 1.025 | Acc: 64.173% (6407/9984)\n",
            "156 782 Loss: 1.026 | Acc: 64.092% (6440/10048)\n",
            "157 782 Loss: 1.025 | Acc: 64.122% (6484/10112)\n",
            "158 782 Loss: 1.025 | Acc: 64.121% (6525/10176)\n",
            "159 782 Loss: 1.025 | Acc: 64.131% (6567/10240)\n",
            "160 782 Loss: 1.024 | Acc: 64.189% (6614/10304)\n",
            "161 782 Loss: 1.024 | Acc: 64.188% (6655/10368)\n",
            "162 782 Loss: 1.024 | Acc: 64.225% (6700/10432)\n",
            "163 782 Loss: 1.025 | Acc: 64.234% (6742/10496)\n",
            "164 782 Loss: 1.025 | Acc: 64.223% (6782/10560)\n",
            "165 782 Loss: 1.025 | Acc: 64.147% (6815/10624)\n",
            "166 782 Loss: 1.026 | Acc: 64.128% (6854/10688)\n",
            "167 782 Loss: 1.026 | Acc: 64.118% (6894/10752)\n",
            "168 782 Loss: 1.026 | Acc: 64.136% (6937/10816)\n",
            "169 782 Loss: 1.025 | Acc: 64.136% (6978/10880)\n",
            "170 782 Loss: 1.025 | Acc: 64.126% (7018/10944)\n",
            "171 782 Loss: 1.026 | Acc: 64.108% (7057/11008)\n",
            "172 782 Loss: 1.025 | Acc: 64.162% (7104/11072)\n",
            "173 782 Loss: 1.025 | Acc: 64.179% (7147/11136)\n",
            "174 782 Loss: 1.025 | Acc: 64.188% (7189/11200)\n",
            "175 782 Loss: 1.026 | Acc: 64.107% (7221/11264)\n",
            "176 782 Loss: 1.027 | Acc: 64.027% (7253/11328)\n",
            "177 782 Loss: 1.027 | Acc: 64.010% (7292/11392)\n",
            "178 782 Loss: 1.027 | Acc: 63.966% (7328/11456)\n",
            "179 782 Loss: 1.027 | Acc: 63.958% (7368/11520)\n",
            "180 782 Loss: 1.027 | Acc: 63.968% (7410/11584)\n",
            "181 782 Loss: 1.027 | Acc: 63.951% (7449/11648)\n",
            "182 782 Loss: 1.027 | Acc: 63.952% (7490/11712)\n",
            "183 782 Loss: 1.026 | Acc: 63.944% (7530/11776)\n",
            "184 782 Loss: 1.025 | Acc: 64.003% (7578/11840)\n",
            "185 782 Loss: 1.025 | Acc: 64.029% (7622/11904)\n",
            "186 782 Loss: 1.025 | Acc: 64.021% (7662/11968)\n",
            "187 782 Loss: 1.026 | Acc: 64.013% (7702/12032)\n",
            "188 782 Loss: 1.025 | Acc: 64.029% (7745/12096)\n",
            "189 782 Loss: 1.024 | Acc: 64.062% (7790/12160)\n",
            "190 782 Loss: 1.024 | Acc: 64.038% (7828/12224)\n",
            "191 782 Loss: 1.024 | Acc: 64.006% (7865/12288)\n",
            "192 782 Loss: 1.023 | Acc: 64.038% (7910/12352)\n",
            "193 782 Loss: 1.022 | Acc: 64.079% (7956/12416)\n",
            "194 782 Loss: 1.022 | Acc: 64.071% (7996/12480)\n",
            "195 782 Loss: 1.022 | Acc: 64.062% (8036/12544)\n",
            "196 782 Loss: 1.023 | Acc: 64.007% (8070/12608)\n",
            "197 782 Loss: 1.023 | Acc: 63.991% (8109/12672)\n",
            "198 782 Loss: 1.023 | Acc: 63.984% (8149/12736)\n",
            "199 782 Loss: 1.024 | Acc: 63.953% (8186/12800)\n",
            "200 782 Loss: 1.024 | Acc: 63.946% (8226/12864)\n",
            "201 782 Loss: 1.024 | Acc: 63.977% (8271/12928)\n",
            "202 782 Loss: 1.023 | Acc: 63.978% (8312/12992)\n",
            "203 782 Loss: 1.023 | Acc: 63.978% (8353/13056)\n",
            "204 782 Loss: 1.023 | Acc: 63.948% (8390/13120)\n",
            "205 782 Loss: 1.023 | Acc: 63.949% (8431/13184)\n",
            "206 782 Loss: 1.024 | Acc: 63.912% (8467/13248)\n",
            "207 782 Loss: 1.025 | Acc: 63.905% (8507/13312)\n",
            "208 782 Loss: 1.025 | Acc: 63.898% (8547/13376)\n",
            "209 782 Loss: 1.024 | Acc: 63.936% (8593/13440)\n",
            "210 782 Loss: 1.023 | Acc: 63.974% (8639/13504)\n",
            "211 782 Loss: 1.025 | Acc: 63.930% (8674/13568)\n",
            "212 782 Loss: 1.026 | Acc: 63.894% (8710/13632)\n",
            "213 782 Loss: 1.026 | Acc: 63.873% (8748/13696)\n",
            "214 782 Loss: 1.025 | Acc: 63.874% (8789/13760)\n",
            "215 782 Loss: 1.026 | Acc: 63.867% (8829/13824)\n",
            "216 782 Loss: 1.024 | Acc: 63.904% (8875/13888)\n",
            "217 782 Loss: 1.024 | Acc: 63.905% (8916/13952)\n",
            "218 782 Loss: 1.023 | Acc: 63.963% (8965/14016)\n",
            "219 782 Loss: 1.024 | Acc: 63.892% (8996/14080)\n",
            "220 782 Loss: 1.024 | Acc: 63.921% (9041/14144)\n",
            "221 782 Loss: 1.025 | Acc: 63.880% (9076/14208)\n",
            "222 782 Loss: 1.024 | Acc: 63.943% (9126/14272)\n",
            "223 782 Loss: 1.025 | Acc: 63.909% (9162/14336)\n",
            "224 782 Loss: 1.025 | Acc: 63.944% (9208/14400)\n",
            "225 782 Loss: 1.026 | Acc: 63.966% (9252/14464)\n",
            "226 782 Loss: 1.026 | Acc: 63.959% (9292/14528)\n",
            "227 782 Loss: 1.026 | Acc: 63.946% (9331/14592)\n",
            "228 782 Loss: 1.026 | Acc: 63.953% (9373/14656)\n",
            "229 782 Loss: 1.026 | Acc: 63.954% (9414/14720)\n",
            "230 782 Loss: 1.027 | Acc: 63.927% (9451/14784)\n",
            "231 782 Loss: 1.026 | Acc: 63.941% (9494/14848)\n",
            "232 782 Loss: 1.026 | Acc: 63.922% (9532/14912)\n",
            "233 782 Loss: 1.027 | Acc: 63.882% (9567/14976)\n",
            "234 782 Loss: 1.026 | Acc: 63.870% (9606/15040)\n",
            "235 782 Loss: 1.028 | Acc: 63.824% (9640/15104)\n",
            "236 782 Loss: 1.027 | Acc: 63.845% (9684/15168)\n",
            "237 782 Loss: 1.028 | Acc: 63.787% (9716/15232)\n",
            "238 782 Loss: 1.028 | Acc: 63.801% (9759/15296)\n",
            "239 782 Loss: 1.027 | Acc: 63.854% (9808/15360)\n",
            "240 782 Loss: 1.028 | Acc: 63.797% (9840/15424)\n",
            "241 782 Loss: 1.029 | Acc: 63.740% (9872/15488)\n",
            "242 782 Loss: 1.028 | Acc: 63.780% (9919/15552)\n",
            "243 782 Loss: 1.028 | Acc: 63.768% (9958/15616)\n",
            "244 782 Loss: 1.027 | Acc: 63.788% (10002/15680)\n",
            "245 782 Loss: 1.027 | Acc: 63.834% (10050/15744)\n",
            "246 782 Loss: 1.028 | Acc: 63.790% (10084/15808)\n",
            "247 782 Loss: 1.027 | Acc: 63.798% (10126/15872)\n",
            "248 782 Loss: 1.027 | Acc: 63.786% (10165/15936)\n",
            "249 782 Loss: 1.027 | Acc: 63.812% (10210/16000)\n",
            "250 782 Loss: 1.027 | Acc: 63.813% (10251/16064)\n",
            "251 782 Loss: 1.026 | Acc: 63.864% (10300/16128)\n",
            "252 782 Loss: 1.026 | Acc: 63.865% (10341/16192)\n",
            "253 782 Loss: 1.026 | Acc: 63.853% (10380/16256)\n",
            "254 782 Loss: 1.026 | Acc: 63.830% (10417/16320)\n",
            "255 782 Loss: 1.026 | Acc: 63.849% (10461/16384)\n",
            "256 782 Loss: 1.025 | Acc: 63.898% (10510/16448)\n",
            "257 782 Loss: 1.025 | Acc: 63.893% (10550/16512)\n",
            "258 782 Loss: 1.025 | Acc: 63.912% (10594/16576)\n",
            "259 782 Loss: 1.026 | Acc: 63.894% (10632/16640)\n",
            "260 782 Loss: 1.026 | Acc: 63.907% (10675/16704)\n",
            "261 782 Loss: 1.025 | Acc: 63.937% (10721/16768)\n",
            "262 782 Loss: 1.024 | Acc: 63.973% (10768/16832)\n",
            "263 782 Loss: 1.025 | Acc: 63.968% (10808/16896)\n",
            "264 782 Loss: 1.025 | Acc: 63.950% (10846/16960)\n",
            "265 782 Loss: 1.026 | Acc: 63.892% (10877/17024)\n",
            "266 782 Loss: 1.026 | Acc: 63.875% (10915/17088)\n",
            "267 782 Loss: 1.026 | Acc: 63.864% (10954/17152)\n",
            "268 782 Loss: 1.026 | Acc: 63.859% (10994/17216)\n",
            "269 782 Loss: 1.026 | Acc: 63.895% (11041/17280)\n",
            "270 782 Loss: 1.025 | Acc: 63.901% (11083/17344)\n",
            "271 782 Loss: 1.026 | Acc: 63.896% (11123/17408)\n",
            "272 782 Loss: 1.026 | Acc: 63.897% (11164/17472)\n",
            "273 782 Loss: 1.026 | Acc: 63.891% (11204/17536)\n",
            "274 782 Loss: 1.025 | Acc: 63.909% (11248/17600)\n",
            "275 782 Loss: 1.025 | Acc: 63.910% (11289/17664)\n",
            "276 782 Loss: 1.026 | Acc: 63.910% (11330/17728)\n",
            "277 782 Loss: 1.026 | Acc: 63.911% (11371/17792)\n",
            "278 782 Loss: 1.026 | Acc: 63.889% (11408/17856)\n",
            "279 782 Loss: 1.026 | Acc: 63.878% (11447/17920)\n",
            "280 782 Loss: 1.026 | Acc: 63.868% (11486/17984)\n",
            "281 782 Loss: 1.025 | Acc: 63.896% (11532/18048)\n",
            "282 782 Loss: 1.026 | Acc: 63.869% (11568/18112)\n",
            "283 782 Loss: 1.025 | Acc: 63.897% (11614/18176)\n",
            "284 782 Loss: 1.025 | Acc: 63.882% (11652/18240)\n",
            "285 782 Loss: 1.026 | Acc: 63.871% (11691/18304)\n",
            "286 782 Loss: 1.026 | Acc: 63.877% (11733/18368)\n",
            "287 782 Loss: 1.027 | Acc: 63.856% (11770/18432)\n",
            "288 782 Loss: 1.027 | Acc: 63.841% (11808/18496)\n",
            "289 782 Loss: 1.029 | Acc: 63.798% (11841/18560)\n",
            "290 782 Loss: 1.028 | Acc: 63.826% (11887/18624)\n",
            "291 782 Loss: 1.029 | Acc: 63.795% (11922/18688)\n",
            "292 782 Loss: 1.029 | Acc: 63.801% (11964/18752)\n",
            "293 782 Loss: 1.029 | Acc: 63.797% (12004/18816)\n",
            "294 782 Loss: 1.030 | Acc: 63.803% (12046/18880)\n",
            "295 782 Loss: 1.030 | Acc: 63.799% (12086/18944)\n",
            "296 782 Loss: 1.030 | Acc: 63.789% (12125/19008)\n",
            "297 782 Loss: 1.029 | Acc: 63.816% (12171/19072)\n",
            "298 782 Loss: 1.029 | Acc: 63.786% (12206/19136)\n",
            "299 782 Loss: 1.030 | Acc: 63.792% (12248/19200)\n",
            "300 782 Loss: 1.029 | Acc: 63.798% (12290/19264)\n",
            "301 782 Loss: 1.030 | Acc: 63.793% (12330/19328)\n",
            "302 782 Loss: 1.030 | Acc: 63.815% (12375/19392)\n",
            "303 782 Loss: 1.030 | Acc: 63.816% (12416/19456)\n",
            "304 782 Loss: 1.031 | Acc: 63.791% (12452/19520)\n",
            "305 782 Loss: 1.030 | Acc: 63.817% (12498/19584)\n",
            "306 782 Loss: 1.030 | Acc: 63.798% (12535/19648)\n",
            "307 782 Loss: 1.029 | Acc: 63.824% (12581/19712)\n",
            "308 782 Loss: 1.030 | Acc: 63.800% (12617/19776)\n",
            "309 782 Loss: 1.030 | Acc: 63.785% (12655/19840)\n",
            "310 782 Loss: 1.031 | Acc: 63.756% (12690/19904)\n",
            "311 782 Loss: 1.031 | Acc: 63.752% (12730/19968)\n",
            "312 782 Loss: 1.031 | Acc: 63.753% (12771/20032)\n",
            "313 782 Loss: 1.031 | Acc: 63.784% (12818/20096)\n",
            "314 782 Loss: 1.030 | Acc: 63.819% (12866/20160)\n",
            "315 782 Loss: 1.030 | Acc: 63.815% (12906/20224)\n",
            "316 782 Loss: 1.031 | Acc: 63.826% (12949/20288)\n",
            "317 782 Loss: 1.031 | Acc: 63.812% (12987/20352)\n",
            "318 782 Loss: 1.031 | Acc: 63.808% (13027/20416)\n",
            "319 782 Loss: 1.031 | Acc: 63.799% (13066/20480)\n",
            "320 782 Loss: 1.031 | Acc: 63.819% (13111/20544)\n",
            "321 782 Loss: 1.031 | Acc: 63.820% (13152/20608)\n",
            "322 782 Loss: 1.031 | Acc: 63.835% (13196/20672)\n",
            "323 782 Loss: 1.031 | Acc: 63.836% (13237/20736)\n",
            "324 782 Loss: 1.031 | Acc: 63.827% (13276/20800)\n",
            "325 782 Loss: 1.030 | Acc: 63.842% (13320/20864)\n",
            "326 782 Loss: 1.031 | Acc: 63.847% (13362/20928)\n",
            "327 782 Loss: 1.030 | Acc: 63.881% (13410/20992)\n",
            "328 782 Loss: 1.030 | Acc: 63.873% (13449/21056)\n",
            "329 782 Loss: 1.030 | Acc: 63.902% (13496/21120)\n",
            "330 782 Loss: 1.030 | Acc: 63.907% (13538/21184)\n",
            "331 782 Loss: 1.030 | Acc: 63.907% (13579/21248)\n",
            "332 782 Loss: 1.030 | Acc: 63.912% (13621/21312)\n",
            "333 782 Loss: 1.030 | Acc: 63.908% (13661/21376)\n",
            "334 782 Loss: 1.030 | Acc: 63.913% (13703/21440)\n",
            "335 782 Loss: 1.030 | Acc: 63.918% (13745/21504)\n",
            "336 782 Loss: 1.029 | Acc: 63.942% (13791/21568)\n",
            "337 782 Loss: 1.029 | Acc: 63.975% (13839/21632)\n",
            "338 782 Loss: 1.029 | Acc: 63.961% (13877/21696)\n",
            "339 782 Loss: 1.029 | Acc: 63.980% (13922/21760)\n",
            "340 782 Loss: 1.030 | Acc: 63.953% (13957/21824)\n",
            "341 782 Loss: 1.030 | Acc: 63.944% (13996/21888)\n",
            "342 782 Loss: 1.030 | Acc: 63.921% (14032/21952)\n",
            "343 782 Loss: 1.030 | Acc: 63.926% (14074/22016)\n",
            "344 782 Loss: 1.030 | Acc: 63.909% (14111/22080)\n",
            "345 782 Loss: 1.031 | Acc: 63.886% (14147/22144)\n",
            "346 782 Loss: 1.031 | Acc: 63.909% (14193/22208)\n",
            "347 782 Loss: 1.031 | Acc: 63.905% (14233/22272)\n",
            "348 782 Loss: 1.031 | Acc: 63.897% (14272/22336)\n",
            "349 782 Loss: 1.031 | Acc: 63.897% (14313/22400)\n",
            "350 782 Loss: 1.031 | Acc: 63.916% (14358/22464)\n",
            "351 782 Loss: 1.031 | Acc: 63.889% (14393/22528)\n",
            "352 782 Loss: 1.031 | Acc: 63.877% (14431/22592)\n",
            "353 782 Loss: 1.031 | Acc: 63.882% (14473/22656)\n",
            "354 782 Loss: 1.031 | Acc: 63.895% (14517/22720)\n",
            "355 782 Loss: 1.030 | Acc: 63.935% (14567/22784)\n",
            "356 782 Loss: 1.031 | Acc: 63.922% (14605/22848)\n",
            "357 782 Loss: 1.031 | Acc: 63.936% (14649/22912)\n",
            "358 782 Loss: 1.032 | Acc: 63.906% (14683/22976)\n",
            "359 782 Loss: 1.033 | Acc: 63.859% (14713/23040)\n",
            "360 782 Loss: 1.032 | Acc: 63.881% (14759/23104)\n",
            "361 782 Loss: 1.033 | Acc: 63.881% (14800/23168)\n",
            "362 782 Loss: 1.032 | Acc: 63.903% (14846/23232)\n",
            "363 782 Loss: 1.031 | Acc: 63.904% (14887/23296)\n",
            "364 782 Loss: 1.032 | Acc: 63.908% (14929/23360)\n",
            "365 782 Loss: 1.032 | Acc: 63.913% (14971/23424)\n",
            "366 782 Loss: 1.031 | Acc: 63.913% (15012/23488)\n",
            "367 782 Loss: 1.031 | Acc: 63.905% (15051/23552)\n",
            "368 782 Loss: 1.030 | Acc: 63.931% (15098/23616)\n",
            "369 782 Loss: 1.031 | Acc: 63.940% (15141/23680)\n",
            "370 782 Loss: 1.031 | Acc: 63.945% (15183/23744)\n",
            "371 782 Loss: 1.031 | Acc: 63.941% (15223/23808)\n",
            "372 782 Loss: 1.030 | Acc: 63.970% (15271/23872)\n",
            "373 782 Loss: 1.030 | Acc: 63.966% (15311/23936)\n",
            "374 782 Loss: 1.029 | Acc: 63.987% (15357/24000)\n",
            "375 782 Loss: 1.030 | Acc: 63.959% (15391/24064)\n",
            "376 782 Loss: 1.029 | Acc: 63.975% (15436/24128)\n",
            "377 782 Loss: 1.029 | Acc: 63.963% (15474/24192)\n",
            "378 782 Loss: 1.029 | Acc: 63.980% (15519/24256)\n",
            "379 782 Loss: 1.029 | Acc: 64.001% (15565/24320)\n",
            "380 782 Loss: 1.029 | Acc: 63.989% (15603/24384)\n",
            "381 782 Loss: 1.029 | Acc: 63.989% (15644/24448)\n",
            "382 782 Loss: 1.028 | Acc: 64.009% (15690/24512)\n",
            "383 782 Loss: 1.029 | Acc: 63.989% (15726/24576)\n",
            "384 782 Loss: 1.029 | Acc: 63.985% (15766/24640)\n",
            "385 782 Loss: 1.029 | Acc: 63.969% (15803/24704)\n",
            "386 782 Loss: 1.029 | Acc: 63.953% (15840/24768)\n",
            "387 782 Loss: 1.029 | Acc: 63.934% (15876/24832)\n",
            "388 782 Loss: 1.029 | Acc: 63.954% (15922/24896)\n",
            "389 782 Loss: 1.029 | Acc: 63.934% (15958/24960)\n",
            "390 782 Loss: 1.029 | Acc: 63.935% (15999/25024)\n",
            "391 782 Loss: 1.029 | Acc: 63.939% (16041/25088)\n",
            "392 782 Loss: 1.029 | Acc: 63.947% (16084/25152)\n",
            "393 782 Loss: 1.029 | Acc: 63.951% (16126/25216)\n",
            "394 782 Loss: 1.028 | Acc: 63.983% (16175/25280)\n",
            "395 782 Loss: 1.028 | Acc: 63.999% (16220/25344)\n",
            "396 782 Loss: 1.028 | Acc: 63.996% (16260/25408)\n",
            "397 782 Loss: 1.028 | Acc: 64.015% (16306/25472)\n",
            "398 782 Loss: 1.027 | Acc: 64.019% (16348/25536)\n",
            "399 782 Loss: 1.027 | Acc: 64.047% (16396/25600)\n",
            "400 782 Loss: 1.027 | Acc: 64.066% (16442/25664)\n",
            "401 782 Loss: 1.027 | Acc: 64.047% (16478/25728)\n",
            "402 782 Loss: 1.027 | Acc: 64.024% (16513/25792)\n",
            "403 782 Loss: 1.029 | Acc: 64.001% (16548/25856)\n",
            "404 782 Loss: 1.028 | Acc: 64.005% (16590/25920)\n",
            "405 782 Loss: 1.029 | Acc: 63.989% (16627/25984)\n",
            "406 782 Loss: 1.030 | Acc: 63.951% (16658/26048)\n",
            "407 782 Loss: 1.030 | Acc: 63.940% (16696/26112)\n",
            "408 782 Loss: 1.030 | Acc: 63.956% (16741/26176)\n",
            "409 782 Loss: 1.029 | Acc: 63.967% (16785/26240)\n",
            "410 782 Loss: 1.030 | Acc: 63.956% (16823/26304)\n",
            "411 782 Loss: 1.029 | Acc: 63.968% (16867/26368)\n",
            "412 782 Loss: 1.029 | Acc: 63.960% (16906/26432)\n",
            "413 782 Loss: 1.029 | Acc: 63.949% (16944/26496)\n",
            "414 782 Loss: 1.029 | Acc: 63.961% (16988/26560)\n",
            "415 782 Loss: 1.029 | Acc: 63.969% (17031/26624)\n",
            "416 782 Loss: 1.029 | Acc: 63.954% (17068/26688)\n",
            "417 782 Loss: 1.030 | Acc: 63.935% (17104/26752)\n",
            "418 782 Loss: 1.030 | Acc: 63.917% (17140/26816)\n",
            "419 782 Loss: 1.030 | Acc: 63.910% (17179/26880)\n",
            "420 782 Loss: 1.030 | Acc: 63.888% (17214/26944)\n",
            "421 782 Loss: 1.030 | Acc: 63.892% (17256/27008)\n",
            "422 782 Loss: 1.031 | Acc: 63.889% (17296/27072)\n",
            "423 782 Loss: 1.030 | Acc: 63.897% (17339/27136)\n",
            "424 782 Loss: 1.030 | Acc: 63.897% (17380/27200)\n",
            "425 782 Loss: 1.029 | Acc: 63.919% (17427/27264)\n",
            "426 782 Loss: 1.029 | Acc: 63.916% (17467/27328)\n",
            "427 782 Loss: 1.029 | Acc: 63.916% (17508/27392)\n",
            "428 782 Loss: 1.029 | Acc: 63.924% (17551/27456)\n",
            "429 782 Loss: 1.029 | Acc: 63.943% (17597/27520)\n",
            "430 782 Loss: 1.029 | Acc: 63.925% (17633/27584)\n",
            "431 782 Loss: 1.030 | Acc: 63.903% (17668/27648)\n",
            "432 782 Loss: 1.030 | Acc: 63.904% (17709/27712)\n",
            "433 782 Loss: 1.030 | Acc: 63.915% (17753/27776)\n",
            "434 782 Loss: 1.030 | Acc: 63.908% (17792/27840)\n",
            "435 782 Loss: 1.030 | Acc: 63.901% (17831/27904)\n",
            "436 782 Loss: 1.030 | Acc: 63.891% (17869/27968)\n",
            "437 782 Loss: 1.030 | Acc: 63.881% (17907/28032)\n",
            "438 782 Loss: 1.030 | Acc: 63.888% (17950/28096)\n",
            "439 782 Loss: 1.030 | Acc: 63.892% (17992/28160)\n",
            "440 782 Loss: 1.030 | Acc: 63.917% (18040/28224)\n",
            "441 782 Loss: 1.030 | Acc: 63.939% (18087/28288)\n",
            "442 782 Loss: 1.030 | Acc: 63.932% (18126/28352)\n",
            "443 782 Loss: 1.030 | Acc: 63.911% (18161/28416)\n",
            "444 782 Loss: 1.031 | Acc: 63.912% (18202/28480)\n",
            "445 782 Loss: 1.031 | Acc: 63.891% (18237/28544)\n",
            "446 782 Loss: 1.031 | Acc: 63.888% (18277/28608)\n",
            "447 782 Loss: 1.031 | Acc: 63.881% (18316/28672)\n",
            "448 782 Loss: 1.031 | Acc: 63.892% (18360/28736)\n",
            "449 782 Loss: 1.031 | Acc: 63.885% (18399/28800)\n",
            "450 782 Loss: 1.031 | Acc: 63.851% (18430/28864)\n",
            "451 782 Loss: 1.030 | Acc: 63.865% (18475/28928)\n",
            "452 782 Loss: 1.030 | Acc: 63.859% (18514/28992)\n",
            "453 782 Loss: 1.031 | Acc: 63.842% (18550/29056)\n",
            "454 782 Loss: 1.030 | Acc: 63.839% (18590/29120)\n",
            "455 782 Loss: 1.030 | Acc: 63.847% (18633/29184)\n",
            "456 782 Loss: 1.031 | Acc: 63.840% (18672/29248)\n",
            "457 782 Loss: 1.030 | Acc: 63.834% (18711/29312)\n",
            "458 782 Loss: 1.030 | Acc: 63.841% (18754/29376)\n",
            "459 782 Loss: 1.030 | Acc: 63.849% (18797/29440)\n",
            "460 782 Loss: 1.030 | Acc: 63.849% (18838/29504)\n",
            "461 782 Loss: 1.030 | Acc: 63.846% (18878/29568)\n",
            "462 782 Loss: 1.031 | Acc: 63.843% (18918/29632)\n",
            "463 782 Loss: 1.031 | Acc: 63.837% (18957/29696)\n",
            "464 782 Loss: 1.030 | Acc: 63.844% (19000/29760)\n",
            "465 782 Loss: 1.031 | Acc: 63.838% (19039/29824)\n",
            "466 782 Loss: 1.031 | Acc: 63.848% (19083/29888)\n",
            "467 782 Loss: 1.030 | Acc: 63.862% (19128/29952)\n",
            "468 782 Loss: 1.031 | Acc: 63.849% (19165/30016)\n",
            "469 782 Loss: 1.031 | Acc: 63.826% (19199/30080)\n",
            "470 782 Loss: 1.031 | Acc: 63.837% (19243/30144)\n",
            "471 782 Loss: 1.031 | Acc: 63.854% (19289/30208)\n",
            "472 782 Loss: 1.031 | Acc: 63.858% (19331/30272)\n",
            "473 782 Loss: 1.030 | Acc: 63.858% (19372/30336)\n",
            "474 782 Loss: 1.030 | Acc: 63.849% (19410/30400)\n",
            "475 782 Loss: 1.030 | Acc: 63.859% (19454/30464)\n",
            "476 782 Loss: 1.030 | Acc: 63.853% (19493/30528)\n",
            "477 782 Loss: 1.031 | Acc: 63.850% (19533/30592)\n",
            "478 782 Loss: 1.031 | Acc: 63.821% (19565/30656)\n",
            "479 782 Loss: 1.031 | Acc: 63.825% (19607/30720)\n",
            "480 782 Loss: 1.031 | Acc: 63.803% (19641/30784)\n",
            "481 782 Loss: 1.031 | Acc: 63.793% (19679/30848)\n",
            "482 782 Loss: 1.032 | Acc: 63.768% (19712/30912)\n",
            "483 782 Loss: 1.032 | Acc: 63.772% (19754/30976)\n",
            "484 782 Loss: 1.031 | Acc: 63.779% (19797/31040)\n",
            "485 782 Loss: 1.032 | Acc: 63.776% (19837/31104)\n",
            "486 782 Loss: 1.032 | Acc: 63.771% (19876/31168)\n",
            "487 782 Loss: 1.032 | Acc: 63.778% (19919/31232)\n",
            "488 782 Loss: 1.032 | Acc: 63.772% (19958/31296)\n",
            "489 782 Loss: 1.032 | Acc: 63.766% (19997/31360)\n",
            "490 782 Loss: 1.032 | Acc: 63.757% (20035/31424)\n",
            "491 782 Loss: 1.032 | Acc: 63.745% (20072/31488)\n",
            "492 782 Loss: 1.032 | Acc: 63.755% (20116/31552)\n",
            "493 782 Loss: 1.032 | Acc: 63.762% (20159/31616)\n",
            "494 782 Loss: 1.032 | Acc: 63.753% (20197/31680)\n",
            "495 782 Loss: 1.032 | Acc: 63.763% (20241/31744)\n",
            "496 782 Loss: 1.032 | Acc: 63.745% (20276/31808)\n",
            "497 782 Loss: 1.032 | Acc: 63.742% (20316/31872)\n",
            "498 782 Loss: 1.032 | Acc: 63.778% (20368/31936)\n",
            "499 782 Loss: 1.032 | Acc: 63.784% (20411/32000)\n",
            "500 782 Loss: 1.032 | Acc: 63.760% (20444/32064)\n",
            "501 782 Loss: 1.033 | Acc: 63.757% (20484/32128)\n",
            "502 782 Loss: 1.033 | Acc: 63.743% (20520/32192)\n",
            "503 782 Loss: 1.033 | Acc: 63.752% (20564/32256)\n",
            "504 782 Loss: 1.033 | Acc: 63.741% (20601/32320)\n",
            "505 782 Loss: 1.032 | Acc: 63.751% (20645/32384)\n",
            "506 782 Loss: 1.032 | Acc: 63.754% (20687/32448)\n",
            "507 782 Loss: 1.032 | Acc: 63.755% (20728/32512)\n",
            "508 782 Loss: 1.033 | Acc: 63.759% (20770/32576)\n",
            "509 782 Loss: 1.032 | Acc: 63.762% (20812/32640)\n",
            "510 782 Loss: 1.033 | Acc: 63.760% (20852/32704)\n",
            "511 782 Loss: 1.033 | Acc: 63.773% (20897/32768)\n",
            "512 782 Loss: 1.033 | Acc: 63.755% (20932/32832)\n",
            "513 782 Loss: 1.033 | Acc: 63.746% (20970/32896)\n",
            "514 782 Loss: 1.033 | Acc: 63.756% (21014/32960)\n",
            "515 782 Loss: 1.033 | Acc: 63.754% (21054/33024)\n",
            "516 782 Loss: 1.033 | Acc: 63.757% (21096/33088)\n",
            "517 782 Loss: 1.033 | Acc: 63.749% (21134/33152)\n",
            "518 782 Loss: 1.033 | Acc: 63.758% (21178/33216)\n",
            "519 782 Loss: 1.034 | Acc: 63.747% (21215/33280)\n",
            "520 782 Loss: 1.034 | Acc: 63.745% (21255/33344)\n",
            "521 782 Loss: 1.033 | Acc: 63.757% (21300/33408)\n",
            "522 782 Loss: 1.034 | Acc: 63.749% (21338/33472)\n",
            "523 782 Loss: 1.033 | Acc: 63.752% (21380/33536)\n",
            "524 782 Loss: 1.034 | Acc: 63.729% (21413/33600)\n",
            "525 782 Loss: 1.034 | Acc: 63.742% (21458/33664)\n",
            "526 782 Loss: 1.034 | Acc: 63.736% (21497/33728)\n",
            "527 782 Loss: 1.034 | Acc: 63.716% (21531/33792)\n",
            "528 782 Loss: 1.035 | Acc: 63.699% (21566/33856)\n",
            "529 782 Loss: 1.035 | Acc: 63.682% (21601/33920)\n",
            "530 782 Loss: 1.035 | Acc: 63.686% (21643/33984)\n",
            "531 782 Loss: 1.035 | Acc: 63.678% (21681/34048)\n",
            "532 782 Loss: 1.036 | Acc: 63.649% (21712/34112)\n",
            "533 782 Loss: 1.036 | Acc: 63.647% (21752/34176)\n",
            "534 782 Loss: 1.036 | Acc: 63.642% (21791/34240)\n",
            "535 782 Loss: 1.036 | Acc: 63.637% (21830/34304)\n",
            "536 782 Loss: 1.036 | Acc: 63.620% (21865/34368)\n",
            "537 782 Loss: 1.036 | Acc: 63.624% (21907/34432)\n",
            "538 782 Loss: 1.036 | Acc: 63.631% (21950/34496)\n",
            "539 782 Loss: 1.036 | Acc: 63.637% (21993/34560)\n",
            "540 782 Loss: 1.036 | Acc: 63.635% (22033/34624)\n",
            "541 782 Loss: 1.036 | Acc: 63.656% (22081/34688)\n",
            "542 782 Loss: 1.036 | Acc: 63.654% (22121/34752)\n",
            "543 782 Loss: 1.035 | Acc: 63.672% (22168/34816)\n",
            "544 782 Loss: 1.035 | Acc: 63.670% (22208/34880)\n",
            "545 782 Loss: 1.035 | Acc: 63.676% (22251/34944)\n",
            "546 782 Loss: 1.035 | Acc: 63.697% (22299/35008)\n",
            "547 782 Loss: 1.036 | Acc: 63.700% (22341/35072)\n",
            "548 782 Loss: 1.036 | Acc: 63.704% (22383/35136)\n",
            "549 782 Loss: 1.035 | Acc: 63.705% (22424/35200)\n",
            "550 782 Loss: 1.035 | Acc: 63.708% (22466/35264)\n",
            "551 782 Loss: 1.035 | Acc: 63.706% (22506/35328)\n",
            "552 782 Loss: 1.035 | Acc: 63.721% (22552/35392)\n",
            "553 782 Loss: 1.035 | Acc: 63.730% (22596/35456)\n",
            "554 782 Loss: 1.035 | Acc: 63.727% (22636/35520)\n",
            "555 782 Loss: 1.035 | Acc: 63.722% (22675/35584)\n",
            "556 782 Loss: 1.036 | Acc: 63.709% (22711/35648)\n",
            "557 782 Loss: 1.035 | Acc: 63.724% (22757/35712)\n",
            "558 782 Loss: 1.035 | Acc: 63.716% (22795/35776)\n",
            "559 782 Loss: 1.035 | Acc: 63.730% (22841/35840)\n",
            "560 782 Loss: 1.035 | Acc: 63.737% (22884/35904)\n",
            "561 782 Loss: 1.034 | Acc: 63.754% (22931/35968)\n",
            "562 782 Loss: 1.034 | Acc: 63.763% (22975/36032)\n",
            "563 782 Loss: 1.034 | Acc: 63.758% (23014/36096)\n",
            "564 782 Loss: 1.035 | Acc: 63.761% (23056/36160)\n",
            "565 782 Loss: 1.034 | Acc: 63.767% (23099/36224)\n",
            "566 782 Loss: 1.035 | Acc: 63.754% (23135/36288)\n",
            "567 782 Loss: 1.035 | Acc: 63.749% (23174/36352)\n",
            "568 782 Loss: 1.035 | Acc: 63.741% (23212/36416)\n",
            "569 782 Loss: 1.035 | Acc: 63.736% (23251/36480)\n",
            "570 782 Loss: 1.035 | Acc: 63.740% (23293/36544)\n",
            "571 782 Loss: 1.035 | Acc: 63.729% (23330/36608)\n",
            "572 782 Loss: 1.035 | Acc: 63.735% (23373/36672)\n",
            "573 782 Loss: 1.035 | Acc: 63.747% (23418/36736)\n",
            "574 782 Loss: 1.035 | Acc: 63.742% (23457/36800)\n",
            "575 782 Loss: 1.035 | Acc: 63.734% (23495/36864)\n",
            "576 782 Loss: 1.035 | Acc: 63.716% (23529/36928)\n",
            "577 782 Loss: 1.035 | Acc: 63.698% (23563/36992)\n",
            "578 782 Loss: 1.035 | Acc: 63.704% (23606/37056)\n",
            "579 782 Loss: 1.035 | Acc: 63.720% (23653/37120)\n",
            "580 782 Loss: 1.035 | Acc: 63.724% (23695/37184)\n",
            "581 782 Loss: 1.035 | Acc: 63.700% (23727/37248)\n",
            "582 782 Loss: 1.035 | Acc: 63.714% (23773/37312)\n",
            "583 782 Loss: 1.035 | Acc: 63.723% (23817/37376)\n",
            "584 782 Loss: 1.035 | Acc: 63.731% (23861/37440)\n",
            "585 782 Loss: 1.035 | Acc: 63.737% (23904/37504)\n",
            "586 782 Loss: 1.035 | Acc: 63.743% (23947/37568)\n",
            "587 782 Loss: 1.035 | Acc: 63.768% (23997/37632)\n",
            "588 782 Loss: 1.035 | Acc: 63.768% (24038/37696)\n",
            "589 782 Loss: 1.036 | Acc: 63.753% (24073/37760)\n",
            "590 782 Loss: 1.036 | Acc: 63.743% (24110/37824)\n",
            "591 782 Loss: 1.036 | Acc: 63.748% (24153/37888)\n",
            "592 782 Loss: 1.036 | Acc: 63.749% (24194/37952)\n",
            "593 782 Loss: 1.035 | Acc: 63.765% (24241/38016)\n",
            "594 782 Loss: 1.035 | Acc: 63.774% (24285/38080)\n",
            "595 782 Loss: 1.035 | Acc: 63.771% (24325/38144)\n",
            "596 782 Loss: 1.035 | Acc: 63.775% (24367/38208)\n",
            "597 782 Loss: 1.035 | Acc: 63.770% (24406/38272)\n",
            "598 782 Loss: 1.035 | Acc: 63.776% (24449/38336)\n",
            "599 782 Loss: 1.035 | Acc: 63.781% (24492/38400)\n",
            "600 782 Loss: 1.035 | Acc: 63.779% (24532/38464)\n",
            "601 782 Loss: 1.035 | Acc: 63.782% (24574/38528)\n",
            "602 782 Loss: 1.035 | Acc: 63.780% (24614/38592)\n",
            "603 782 Loss: 1.035 | Acc: 63.788% (24658/38656)\n",
            "604 782 Loss: 1.035 | Acc: 63.773% (24693/38720)\n",
            "605 782 Loss: 1.035 | Acc: 63.774% (24734/38784)\n",
            "606 782 Loss: 1.035 | Acc: 63.772% (24774/38848)\n",
            "607 782 Loss: 1.035 | Acc: 63.772% (24815/38912)\n",
            "608 782 Loss: 1.035 | Acc: 63.765% (24853/38976)\n",
            "609 782 Loss: 1.035 | Acc: 63.778% (24899/39040)\n",
            "610 782 Loss: 1.035 | Acc: 63.781% (24941/39104)\n",
            "611 782 Loss: 1.034 | Acc: 63.794% (24987/39168)\n",
            "612 782 Loss: 1.034 | Acc: 63.800% (25030/39232)\n",
            "613 782 Loss: 1.034 | Acc: 63.805% (25073/39296)\n",
            "614 782 Loss: 1.034 | Acc: 63.796% (25110/39360)\n",
            "615 782 Loss: 1.034 | Acc: 63.783% (25146/39424)\n",
            "616 782 Loss: 1.034 | Acc: 63.781% (25186/39488)\n",
            "617 782 Loss: 1.034 | Acc: 63.782% (25227/39552)\n",
            "618 782 Loss: 1.034 | Acc: 63.780% (25267/39616)\n",
            "619 782 Loss: 1.034 | Acc: 63.768% (25303/39680)\n",
            "620 782 Loss: 1.034 | Acc: 63.796% (25355/39744)\n",
            "621 782 Loss: 1.034 | Acc: 63.799% (25397/39808)\n",
            "622 782 Loss: 1.034 | Acc: 63.797% (25437/39872)\n",
            "623 782 Loss: 1.034 | Acc: 63.797% (25478/39936)\n",
            "624 782 Loss: 1.034 | Acc: 63.803% (25521/40000)\n",
            "625 782 Loss: 1.034 | Acc: 63.798% (25560/40064)\n",
            "626 782 Loss: 1.034 | Acc: 63.808% (25605/40128)\n",
            "627 782 Loss: 1.033 | Acc: 63.819% (25650/40192)\n",
            "628 782 Loss: 1.033 | Acc: 63.827% (25694/40256)\n",
            "629 782 Loss: 1.033 | Acc: 63.834% (25738/40320)\n",
            "630 782 Loss: 1.034 | Acc: 63.817% (25772/40384)\n",
            "631 782 Loss: 1.034 | Acc: 63.810% (25810/40448)\n",
            "632 782 Loss: 1.034 | Acc: 63.811% (25851/40512)\n",
            "633 782 Loss: 1.034 | Acc: 63.799% (25887/40576)\n",
            "634 782 Loss: 1.034 | Acc: 63.794% (25926/40640)\n",
            "635 782 Loss: 1.034 | Acc: 63.792% (25966/40704)\n",
            "636 782 Loss: 1.034 | Acc: 63.800% (26010/40768)\n",
            "637 782 Loss: 1.034 | Acc: 63.798% (26050/40832)\n",
            "638 782 Loss: 1.034 | Acc: 63.789% (26087/40896)\n",
            "639 782 Loss: 1.034 | Acc: 63.794% (26130/40960)\n",
            "640 782 Loss: 1.034 | Acc: 63.785% (26167/41024)\n",
            "641 782 Loss: 1.034 | Acc: 63.790% (26210/41088)\n",
            "642 782 Loss: 1.034 | Acc: 63.783% (26248/41152)\n",
            "643 782 Loss: 1.034 | Acc: 63.786% (26290/41216)\n",
            "644 782 Loss: 1.034 | Acc: 63.784% (26330/41280)\n",
            "645 782 Loss: 1.034 | Acc: 63.787% (26372/41344)\n",
            "646 782 Loss: 1.033 | Acc: 63.790% (26414/41408)\n",
            "647 782 Loss: 1.034 | Acc: 63.795% (26457/41472)\n",
            "648 782 Loss: 1.034 | Acc: 63.788% (26495/41536)\n",
            "649 782 Loss: 1.033 | Acc: 63.791% (26537/41600)\n",
            "650 782 Loss: 1.034 | Acc: 63.779% (26573/41664)\n",
            "651 782 Loss: 1.034 | Acc: 63.770% (26610/41728)\n",
            "652 782 Loss: 1.034 | Acc: 63.768% (26650/41792)\n",
            "653 782 Loss: 1.034 | Acc: 63.769% (26691/41856)\n",
            "654 782 Loss: 1.034 | Acc: 63.783% (26738/41920)\n",
            "655 782 Loss: 1.034 | Acc: 63.779% (26777/41984)\n",
            "656 782 Loss: 1.033 | Acc: 63.789% (26822/42048)\n",
            "657 782 Loss: 1.034 | Acc: 63.778% (26858/42112)\n",
            "658 782 Loss: 1.033 | Acc: 63.790% (26904/42176)\n",
            "659 782 Loss: 1.033 | Acc: 63.783% (26942/42240)\n",
            "660 782 Loss: 1.033 | Acc: 63.774% (26979/42304)\n",
            "661 782 Loss: 1.033 | Acc: 63.779% (27022/42368)\n",
            "662 782 Loss: 1.033 | Acc: 63.794% (27069/42432)\n",
            "663 782 Loss: 1.033 | Acc: 63.797% (27111/42496)\n",
            "664 782 Loss: 1.033 | Acc: 63.795% (27151/42560)\n",
            "665 782 Loss: 1.033 | Acc: 63.795% (27192/42624)\n",
            "666 782 Loss: 1.033 | Acc: 63.798% (27234/42688)\n",
            "667 782 Loss: 1.033 | Acc: 63.786% (27270/42752)\n",
            "668 782 Loss: 1.033 | Acc: 63.768% (27303/42816)\n",
            "669 782 Loss: 1.033 | Acc: 63.762% (27341/42880)\n",
            "670 782 Loss: 1.033 | Acc: 63.760% (27381/42944)\n",
            "671 782 Loss: 1.033 | Acc: 63.763% (27423/43008)\n",
            "672 782 Loss: 1.033 | Acc: 63.754% (27460/43072)\n",
            "673 782 Loss: 1.033 | Acc: 63.750% (27499/43136)\n",
            "674 782 Loss: 1.033 | Acc: 63.757% (27543/43200)\n",
            "675 782 Loss: 1.033 | Acc: 63.746% (27579/43264)\n",
            "676 782 Loss: 1.033 | Acc: 63.746% (27620/43328)\n",
            "677 782 Loss: 1.033 | Acc: 63.735% (27656/43392)\n",
            "678 782 Loss: 1.033 | Acc: 63.729% (27694/43456)\n",
            "679 782 Loss: 1.033 | Acc: 63.713% (27728/43520)\n",
            "680 782 Loss: 1.034 | Acc: 63.707% (27766/43584)\n",
            "681 782 Loss: 1.034 | Acc: 63.696% (27802/43648)\n",
            "682 782 Loss: 1.034 | Acc: 63.703% (27846/43712)\n",
            "683 782 Loss: 1.033 | Acc: 63.697% (27884/43776)\n",
            "684 782 Loss: 1.033 | Acc: 63.707% (27929/43840)\n",
            "685 782 Loss: 1.033 | Acc: 63.721% (27976/43904)\n",
            "686 782 Loss: 1.033 | Acc: 63.721% (28017/43968)\n",
            "687 782 Loss: 1.033 | Acc: 63.720% (28057/44032)\n",
            "688 782 Loss: 1.033 | Acc: 63.716% (28096/44096)\n",
            "689 782 Loss: 1.033 | Acc: 63.702% (28131/44160)\n",
            "690 782 Loss: 1.033 | Acc: 63.685% (28164/44224)\n",
            "691 782 Loss: 1.034 | Acc: 63.674% (28200/44288)\n",
            "692 782 Loss: 1.034 | Acc: 63.668% (28238/44352)\n",
            "693 782 Loss: 1.034 | Acc: 63.680% (28284/44416)\n",
            "694 782 Loss: 1.034 | Acc: 63.678% (28324/44480)\n",
            "695 782 Loss: 1.034 | Acc: 63.683% (28367/44544)\n",
            "696 782 Loss: 1.033 | Acc: 63.697% (28414/44608)\n",
            "697 782 Loss: 1.033 | Acc: 63.700% (28456/44672)\n",
            "698 782 Loss: 1.033 | Acc: 63.707% (28500/44736)\n",
            "699 782 Loss: 1.033 | Acc: 63.717% (28545/44800)\n",
            "700 782 Loss: 1.034 | Acc: 63.708% (28582/44864)\n",
            "701 782 Loss: 1.034 | Acc: 63.702% (28620/44928)\n",
            "702 782 Loss: 1.034 | Acc: 63.709% (28664/44992)\n",
            "703 782 Loss: 1.034 | Acc: 63.705% (28703/45056)\n",
            "704 782 Loss: 1.034 | Acc: 63.706% (28744/45120)\n",
            "705 782 Loss: 1.034 | Acc: 63.700% (28782/45184)\n",
            "706 782 Loss: 1.034 | Acc: 63.704% (28825/45248)\n",
            "707 782 Loss: 1.034 | Acc: 63.698% (28863/45312)\n",
            "708 782 Loss: 1.034 | Acc: 63.714% (28911/45376)\n",
            "709 782 Loss: 1.034 | Acc: 63.726% (28957/45440)\n",
            "710 782 Loss: 1.034 | Acc: 63.713% (28992/45504)\n",
            "711 782 Loss: 1.034 | Acc: 63.722% (29037/45568)\n",
            "712 782 Loss: 1.034 | Acc: 63.725% (29079/45632)\n",
            "713 782 Loss: 1.034 | Acc: 63.719% (29117/45696)\n",
            "714 782 Loss: 1.034 | Acc: 63.722% (29159/45760)\n",
            "715 782 Loss: 1.034 | Acc: 63.731% (29204/45824)\n",
            "716 782 Loss: 1.034 | Acc: 63.725% (29242/45888)\n",
            "717 782 Loss: 1.034 | Acc: 63.732% (29286/45952)\n",
            "718 782 Loss: 1.034 | Acc: 63.741% (29331/46016)\n",
            "719 782 Loss: 1.034 | Acc: 63.748% (29375/46080)\n",
            "720 782 Loss: 1.034 | Acc: 63.759% (29421/46144)\n",
            "721 782 Loss: 1.033 | Acc: 63.768% (29466/46208)\n",
            "722 782 Loss: 1.033 | Acc: 63.771% (29508/46272)\n",
            "723 782 Loss: 1.033 | Acc: 63.773% (29550/46336)\n",
            "724 782 Loss: 1.033 | Acc: 63.780% (29594/46400)\n",
            "725 782 Loss: 1.033 | Acc: 63.793% (29641/46464)\n",
            "726 782 Loss: 1.033 | Acc: 63.779% (29675/46528)\n",
            "727 782 Loss: 1.033 | Acc: 63.777% (29715/46592)\n",
            "728 782 Loss: 1.033 | Acc: 63.769% (29752/46656)\n",
            "729 782 Loss: 1.034 | Acc: 63.763% (29790/46720)\n",
            "730 782 Loss: 1.034 | Acc: 63.772% (29835/46784)\n",
            "731 782 Loss: 1.034 | Acc: 63.768% (29874/46848)\n",
            "732 782 Loss: 1.034 | Acc: 63.766% (29914/46912)\n",
            "733 782 Loss: 1.034 | Acc: 63.769% (29956/46976)\n",
            "734 782 Loss: 1.034 | Acc: 63.759% (29992/47040)\n",
            "735 782 Loss: 1.034 | Acc: 63.767% (30037/47104)\n",
            "736 782 Loss: 1.034 | Acc: 63.761% (30075/47168)\n",
            "737 782 Loss: 1.034 | Acc: 63.753% (30112/47232)\n",
            "738 782 Loss: 1.034 | Acc: 63.745% (30149/47296)\n",
            "739 782 Loss: 1.034 | Acc: 63.761% (30197/47360)\n",
            "740 782 Loss: 1.034 | Acc: 63.763% (30239/47424)\n",
            "741 782 Loss: 1.034 | Acc: 63.766% (30281/47488)\n",
            "742 782 Loss: 1.034 | Acc: 63.758% (30318/47552)\n",
            "743 782 Loss: 1.034 | Acc: 63.758% (30359/47616)\n",
            "744 782 Loss: 1.034 | Acc: 63.763% (30402/47680)\n",
            "745 782 Loss: 1.034 | Acc: 63.757% (30440/47744)\n",
            "746 782 Loss: 1.034 | Acc: 63.763% (30484/47808)\n",
            "747 782 Loss: 1.034 | Acc: 63.764% (30525/47872)\n",
            "748 782 Loss: 1.034 | Acc: 63.758% (30563/47936)\n",
            "749 782 Loss: 1.034 | Acc: 63.754% (30602/48000)\n",
            "750 782 Loss: 1.034 | Acc: 63.746% (30639/48064)\n",
            "751 782 Loss: 1.034 | Acc: 63.757% (30685/48128)\n",
            "752 782 Loss: 1.034 | Acc: 63.753% (30724/48192)\n",
            "753 782 Loss: 1.034 | Acc: 63.754% (30765/48256)\n",
            "754 782 Loss: 1.034 | Acc: 63.760% (30809/48320)\n",
            "755 782 Loss: 1.033 | Acc: 63.779% (30859/48384)\n",
            "756 782 Loss: 1.034 | Acc: 63.788% (30904/48448)\n",
            "757 782 Loss: 1.034 | Acc: 63.786% (30944/48512)\n",
            "758 782 Loss: 1.033 | Acc: 63.791% (30987/48576)\n",
            "759 782 Loss: 1.033 | Acc: 63.791% (31028/48640)\n",
            "760 782 Loss: 1.033 | Acc: 63.804% (31075/48704)\n",
            "761 782 Loss: 1.034 | Acc: 63.800% (31114/48768)\n",
            "762 782 Loss: 1.034 | Acc: 63.798% (31154/48832)\n",
            "763 782 Loss: 1.034 | Acc: 63.786% (31189/48896)\n",
            "764 782 Loss: 1.034 | Acc: 63.783% (31228/48960)\n",
            "765 782 Loss: 1.034 | Acc: 63.765% (31260/49024)\n",
            "766 782 Loss: 1.034 | Acc: 63.763% (31300/49088)\n",
            "767 782 Loss: 1.035 | Acc: 63.749% (31334/49152)\n",
            "768 782 Loss: 1.034 | Acc: 63.758% (31379/49216)\n",
            "769 782 Loss: 1.034 | Acc: 63.758% (31420/49280)\n",
            "770 782 Loss: 1.035 | Acc: 63.744% (31454/49344)\n",
            "771 782 Loss: 1.035 | Acc: 63.749% (31497/49408)\n",
            "772 782 Loss: 1.035 | Acc: 63.739% (31533/49472)\n",
            "773 782 Loss: 1.035 | Acc: 63.744% (31576/49536)\n",
            "774 782 Loss: 1.035 | Acc: 63.752% (31621/49600)\n",
            "775 782 Loss: 1.035 | Acc: 63.756% (31664/49664)\n",
            "776 782 Loss: 1.035 | Acc: 63.761% (31707/49728)\n",
            "777 782 Loss: 1.035 | Acc: 63.751% (31743/49792)\n",
            "778 782 Loss: 1.035 | Acc: 63.756% (31786/49856)\n",
            "779 782 Loss: 1.035 | Acc: 63.738% (31818/49920)\n",
            "780 782 Loss: 1.035 | Acc: 63.742% (31861/49984)\n",
            "781 782 Loss: 1.034 | Acc: 63.748% (31874/50000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-LL6fiP0mJm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b3d31e9e-71de-4dab-f69e-7ebeb23fddbf"
      },
      "source": [
        "test()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 157 Loss: 1.021 | Acc: 60.938% (39/64)\n",
            "1 157 Loss: 0.944 | Acc: 64.062% (82/128)\n",
            "2 157 Loss: 0.993 | Acc: 64.583% (124/192)\n",
            "3 157 Loss: 0.999 | Acc: 65.234% (167/256)\n",
            "4 157 Loss: 1.052 | Acc: 62.812% (201/320)\n",
            "5 157 Loss: 1.044 | Acc: 64.583% (248/384)\n",
            "6 157 Loss: 1.052 | Acc: 64.732% (290/448)\n",
            "7 157 Loss: 1.051 | Acc: 63.867% (327/512)\n",
            "8 157 Loss: 1.045 | Acc: 64.583% (372/576)\n",
            "9 157 Loss: 1.037 | Acc: 65.469% (419/640)\n",
            "10 157 Loss: 1.062 | Acc: 65.057% (458/704)\n",
            "11 157 Loss: 1.073 | Acc: 64.974% (499/768)\n",
            "12 157 Loss: 1.055 | Acc: 65.024% (541/832)\n",
            "13 157 Loss: 1.049 | Acc: 65.290% (585/896)\n",
            "14 157 Loss: 1.042 | Acc: 65.312% (627/960)\n",
            "15 157 Loss: 1.047 | Acc: 64.746% (663/1024)\n",
            "16 157 Loss: 1.049 | Acc: 64.614% (703/1088)\n",
            "17 157 Loss: 1.046 | Acc: 64.757% (746/1152)\n",
            "18 157 Loss: 1.045 | Acc: 64.885% (789/1216)\n",
            "19 157 Loss: 1.056 | Acc: 64.766% (829/1280)\n",
            "20 157 Loss: 1.065 | Acc: 64.137% (862/1344)\n",
            "21 157 Loss: 1.066 | Acc: 63.920% (900/1408)\n",
            "22 157 Loss: 1.072 | Acc: 63.655% (937/1472)\n",
            "23 157 Loss: 1.077 | Acc: 63.477% (975/1536)\n",
            "24 157 Loss: 1.080 | Acc: 63.125% (1010/1600)\n",
            "25 157 Loss: 1.075 | Acc: 63.341% (1054/1664)\n",
            "26 157 Loss: 1.077 | Acc: 63.252% (1093/1728)\n",
            "27 157 Loss: 1.086 | Acc: 63.114% (1131/1792)\n",
            "28 157 Loss: 1.092 | Acc: 62.931% (1168/1856)\n",
            "29 157 Loss: 1.092 | Acc: 63.021% (1210/1920)\n",
            "30 157 Loss: 1.087 | Acc: 63.155% (1253/1984)\n",
            "31 157 Loss: 1.087 | Acc: 63.184% (1294/2048)\n",
            "32 157 Loss: 1.087 | Acc: 62.973% (1330/2112)\n",
            "33 157 Loss: 1.079 | Acc: 63.235% (1376/2176)\n",
            "34 157 Loss: 1.076 | Acc: 63.348% (1419/2240)\n",
            "35 157 Loss: 1.081 | Acc: 62.977% (1451/2304)\n",
            "36 157 Loss: 1.085 | Acc: 62.965% (1491/2368)\n",
            "37 157 Loss: 1.089 | Acc: 62.829% (1528/2432)\n",
            "38 157 Loss: 1.091 | Acc: 62.660% (1564/2496)\n",
            "39 157 Loss: 1.097 | Acc: 62.305% (1595/2560)\n",
            "40 157 Loss: 1.094 | Acc: 62.424% (1638/2624)\n",
            "41 157 Loss: 1.093 | Acc: 62.463% (1679/2688)\n",
            "42 157 Loss: 1.087 | Acc: 62.500% (1720/2752)\n",
            "43 157 Loss: 1.085 | Acc: 62.536% (1761/2816)\n",
            "44 157 Loss: 1.084 | Acc: 62.535% (1801/2880)\n",
            "45 157 Loss: 1.082 | Acc: 62.534% (1841/2944)\n",
            "46 157 Loss: 1.082 | Acc: 62.566% (1882/3008)\n",
            "47 157 Loss: 1.086 | Acc: 62.337% (1915/3072)\n",
            "48 157 Loss: 1.085 | Acc: 62.436% (1958/3136)\n",
            "49 157 Loss: 1.085 | Acc: 62.375% (1996/3200)\n",
            "50 157 Loss: 1.088 | Acc: 62.255% (2032/3264)\n",
            "51 157 Loss: 1.091 | Acc: 62.260% (2072/3328)\n",
            "52 157 Loss: 1.093 | Acc: 62.294% (2113/3392)\n",
            "53 157 Loss: 1.094 | Acc: 62.326% (2154/3456)\n",
            "54 157 Loss: 1.096 | Acc: 62.273% (2192/3520)\n",
            "55 157 Loss: 1.092 | Acc: 62.277% (2232/3584)\n",
            "56 157 Loss: 1.094 | Acc: 62.144% (2267/3648)\n",
            "57 157 Loss: 1.096 | Acc: 62.042% (2303/3712)\n",
            "58 157 Loss: 1.094 | Acc: 62.129% (2346/3776)\n",
            "59 157 Loss: 1.097 | Acc: 62.083% (2384/3840)\n",
            "60 157 Loss: 1.097 | Acc: 62.141% (2426/3904)\n",
            "61 157 Loss: 1.094 | Acc: 62.298% (2472/3968)\n",
            "62 157 Loss: 1.092 | Acc: 62.277% (2511/4032)\n",
            "63 157 Loss: 1.093 | Acc: 62.158% (2546/4096)\n",
            "64 157 Loss: 1.094 | Acc: 62.115% (2584/4160)\n",
            "65 157 Loss: 1.092 | Acc: 62.169% (2626/4224)\n",
            "66 157 Loss: 1.091 | Acc: 62.197% (2667/4288)\n",
            "67 157 Loss: 1.090 | Acc: 62.178% (2706/4352)\n",
            "68 157 Loss: 1.090 | Acc: 62.206% (2747/4416)\n",
            "69 157 Loss: 1.092 | Acc: 62.188% (2786/4480)\n",
            "70 157 Loss: 1.092 | Acc: 62.148% (2824/4544)\n",
            "71 157 Loss: 1.090 | Acc: 62.305% (2871/4608)\n",
            "72 157 Loss: 1.090 | Acc: 62.286% (2910/4672)\n",
            "73 157 Loss: 1.091 | Acc: 62.226% (2947/4736)\n",
            "74 157 Loss: 1.093 | Acc: 62.208% (2986/4800)\n",
            "75 157 Loss: 1.093 | Acc: 62.171% (3024/4864)\n",
            "76 157 Loss: 1.093 | Acc: 62.175% (3064/4928)\n",
            "77 157 Loss: 1.093 | Acc: 62.099% (3100/4992)\n",
            "78 157 Loss: 1.091 | Acc: 62.184% (3144/5056)\n",
            "79 157 Loss: 1.089 | Acc: 62.285% (3189/5120)\n",
            "80 157 Loss: 1.089 | Acc: 62.249% (3227/5184)\n",
            "81 157 Loss: 1.089 | Acc: 62.233% (3266/5248)\n",
            "82 157 Loss: 1.088 | Acc: 62.236% (3306/5312)\n",
            "83 157 Loss: 1.089 | Acc: 62.258% (3347/5376)\n",
            "84 157 Loss: 1.090 | Acc: 62.151% (3381/5440)\n",
            "85 157 Loss: 1.093 | Acc: 62.046% (3415/5504)\n",
            "86 157 Loss: 1.096 | Acc: 61.961% (3450/5568)\n",
            "87 157 Loss: 1.096 | Acc: 61.914% (3487/5632)\n",
            "88 157 Loss: 1.098 | Acc: 61.868% (3524/5696)\n",
            "89 157 Loss: 1.100 | Acc: 61.771% (3558/5760)\n",
            "90 157 Loss: 1.097 | Acc: 61.882% (3604/5824)\n",
            "91 157 Loss: 1.096 | Acc: 61.923% (3646/5888)\n",
            "92 157 Loss: 1.094 | Acc: 62.030% (3692/5952)\n",
            "93 157 Loss: 1.094 | Acc: 62.051% (3733/6016)\n",
            "94 157 Loss: 1.094 | Acc: 62.039% (3772/6080)\n",
            "95 157 Loss: 1.095 | Acc: 62.044% (3812/6144)\n",
            "96 157 Loss: 1.093 | Acc: 62.065% (3853/6208)\n",
            "97 157 Loss: 1.091 | Acc: 62.149% (3898/6272)\n",
            "98 157 Loss: 1.091 | Acc: 62.137% (3937/6336)\n",
            "99 157 Loss: 1.091 | Acc: 62.188% (3980/6400)\n",
            "100 157 Loss: 1.091 | Acc: 62.222% (4022/6464)\n",
            "101 157 Loss: 1.091 | Acc: 62.224% (4062/6528)\n",
            "102 157 Loss: 1.091 | Acc: 62.272% (4105/6592)\n",
            "103 157 Loss: 1.091 | Acc: 62.245% (4143/6656)\n",
            "104 157 Loss: 1.089 | Acc: 62.351% (4190/6720)\n",
            "105 157 Loss: 1.091 | Acc: 62.338% (4229/6784)\n",
            "106 157 Loss: 1.092 | Acc: 62.310% (4267/6848)\n",
            "107 157 Loss: 1.094 | Acc: 62.211% (4300/6912)\n",
            "108 157 Loss: 1.094 | Acc: 62.142% (4335/6976)\n",
            "109 157 Loss: 1.093 | Acc: 62.188% (4378/7040)\n",
            "110 157 Loss: 1.092 | Acc: 62.204% (4419/7104)\n",
            "111 157 Loss: 1.093 | Acc: 62.207% (4459/7168)\n",
            "112 157 Loss: 1.095 | Acc: 62.113% (4492/7232)\n",
            "113 157 Loss: 1.093 | Acc: 62.198% (4538/7296)\n",
            "114 157 Loss: 1.093 | Acc: 62.147% (4574/7360)\n",
            "115 157 Loss: 1.096 | Acc: 62.096% (4610/7424)\n",
            "116 157 Loss: 1.097 | Acc: 62.126% (4652/7488)\n",
            "117 157 Loss: 1.097 | Acc: 62.142% (4693/7552)\n",
            "118 157 Loss: 1.097 | Acc: 62.119% (4731/7616)\n",
            "119 157 Loss: 1.097 | Acc: 62.070% (4767/7680)\n",
            "120 157 Loss: 1.099 | Acc: 62.035% (4804/7744)\n",
            "121 157 Loss: 1.099 | Acc: 62.026% (4843/7808)\n",
            "122 157 Loss: 1.101 | Acc: 61.928% (4875/7872)\n",
            "123 157 Loss: 1.100 | Acc: 61.996% (4920/7936)\n",
            "124 157 Loss: 1.101 | Acc: 62.013% (4961/8000)\n",
            "125 157 Loss: 1.101 | Acc: 61.992% (4999/8064)\n",
            "126 157 Loss: 1.103 | Acc: 61.971% (5037/8128)\n",
            "127 157 Loss: 1.103 | Acc: 61.938% (5074/8192)\n",
            "128 157 Loss: 1.104 | Acc: 61.894% (5110/8256)\n",
            "129 157 Loss: 1.104 | Acc: 61.911% (5151/8320)\n",
            "130 157 Loss: 1.105 | Acc: 61.868% (5187/8384)\n",
            "131 157 Loss: 1.105 | Acc: 61.884% (5228/8448)\n",
            "132 157 Loss: 1.104 | Acc: 61.889% (5268/8512)\n",
            "133 157 Loss: 1.104 | Acc: 61.894% (5308/8576)\n",
            "134 157 Loss: 1.105 | Acc: 61.863% (5345/8640)\n",
            "135 157 Loss: 1.107 | Acc: 61.799% (5379/8704)\n",
            "136 157 Loss: 1.107 | Acc: 61.759% (5415/8768)\n",
            "137 157 Loss: 1.106 | Acc: 61.775% (5456/8832)\n",
            "138 157 Loss: 1.107 | Acc: 61.792% (5497/8896)\n",
            "139 157 Loss: 1.107 | Acc: 61.808% (5538/8960)\n",
            "140 157 Loss: 1.108 | Acc: 61.780% (5575/9024)\n",
            "141 157 Loss: 1.105 | Acc: 61.873% (5623/9088)\n",
            "142 157 Loss: 1.106 | Acc: 61.888% (5664/9152)\n",
            "143 157 Loss: 1.105 | Acc: 61.903% (5705/9216)\n",
            "144 157 Loss: 1.103 | Acc: 61.972% (5751/9280)\n",
            "145 157 Loss: 1.104 | Acc: 61.943% (5788/9344)\n",
            "146 157 Loss: 1.104 | Acc: 61.947% (5828/9408)\n",
            "147 157 Loss: 1.104 | Acc: 61.962% (5869/9472)\n",
            "148 157 Loss: 1.101 | Acc: 62.039% (5916/9536)\n",
            "149 157 Loss: 1.102 | Acc: 62.042% (5956/9600)\n",
            "150 157 Loss: 1.104 | Acc: 61.952% (5987/9664)\n",
            "151 157 Loss: 1.107 | Acc: 61.914% (6023/9728)\n",
            "152 157 Loss: 1.109 | Acc: 61.816% (6053/9792)\n",
            "153 157 Loss: 1.110 | Acc: 61.810% (6092/9856)\n",
            "154 157 Loss: 1.110 | Acc: 61.804% (6131/9920)\n",
            "155 157 Loss: 1.109 | Acc: 61.849% (6175/9984)\n",
            "156 157 Loss: 1.112 | Acc: 61.820% (6182/10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTT_Yvk_1JUM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
