{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch - Part6 - Transfer-Learning-Handson.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "arMUdLUgrKhr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we are going to work on ant vs bee classification\n",
        "# download data and move it to a `data/` directory\n",
        "\n",
        "!mkdir data\n",
        "!wget https://download.pytorch.org/tutorial/hymenoptera_data.zip\n",
        "!unzip hymenoptera_data.zip\n",
        "!mv hymenoptera_data/ data/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKSJKHTKnKWy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, models, datasets\n",
        "\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqWqddv5q8vY",
        "colab_type": "text"
      },
      "source": [
        "# Dataset & DataLoader :\n",
        "\n",
        "train_folder = \"data/hymenoptera_data/train\"\n",
        "\n",
        "val_folder = \"data/hymenoptera_data/val\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKpyaJn2squz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_folder = \"data/hymenoptera_data/train\"\n",
        "\n",
        "val_data_folder = \"data/hymenoptera_data/val\"\n",
        "\n",
        "transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
        "                                 transforms.ToTensor(),\n",
        "                                 transforms.Normalize([0.5,0.5,0.5], [0.5,0.5,0.5])])\n",
        "\n",
        "#create train and val dataset\n",
        "train_dataset = datasets.ImageFolder(train_data_folder, transform=transform)\n",
        "val_dataset = datasets.ImageFolder(val_data_folder, transform=transform)\n",
        "\n",
        "# iterable dataloader\n",
        "trainloader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=1)\n",
        "valloader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpQ6WShg8Flk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f\"num of sample in train_dataset : {len(train_dataset)}\")\n",
        "print(f\"num of sample in val dataset : {len(val_dataset)}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIRGnd59srGK",
        "colab_type": "text"
      },
      "source": [
        "# model fine-tune :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkaTRvHirAg_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lets use resnet18 model pretrained on imagenet and finetune for our two class classification\n",
        "\n",
        "model = torchvision.models.resnet18(pretrained=True)\n",
        "#num of infeatures in fully connected layer\n",
        "num_features = model.fc.in_features\n",
        "# change number of classes from 1000 to 2\n",
        "model.fc = nn.Linear(num_features, 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUhUw_sK1XeI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#lets look at the model architechture\n",
        "model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjHVpU4D0BsW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.7)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQWLbhrxoAEn",
        "colab_type": "text"
      },
      "source": [
        "# training process :\n",
        "\n",
        "inputs --> forward_pass --> loss_compute --> backward_pass --> update_weights --> repeat"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6L88UUtB3t01",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StQW7j6An4IT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_of_epochs = 10\n",
        "\n",
        "for epoch in range(num_of_epochs):\n",
        "  model = model.to(device)\n",
        "  # set to train mode\n",
        "  model.train()\n",
        "  running_loss = 0.0\n",
        "  correct_preds = 0\n",
        "  for images, labels in trainloader:\n",
        "    #access all data\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "    # erase all accumulated gradients\n",
        "    optimizer.zero_grad()\n",
        "    # predictions from model\n",
        "    output = model(images)\n",
        "    # calculate the loss\n",
        "    loss = criterion(output, labels)\n",
        "    #backprop\n",
        "    loss.backward()\n",
        "    #update_weights\n",
        "    optimizer.step()\n",
        "    # predicion on train dataset\n",
        "    _, preds = torch.max(output,1)\n",
        "\n",
        "    running_loss += loss.item()\n",
        "    correct_preds+=torch.sum(preds == labels.data)\n",
        "\n",
        "  print(f\"finished epoch {epoch}\")\n",
        "  print(f\"epoch-{epoch}, loss : {running_loss}, accuracy : {correct_preds.double()/len(train_dataset)}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vqyo1fUa9vp3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classes = train_dataset.classes\n",
        "print(classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFrIhRZFARGY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# inference on a sinlge validation data in validation dataset.\n",
        "test_image = val_dataset[-1][0].unsqueeze(0)\n",
        "test_image_label = val_dataset[-1][1]\n",
        "# print(test_image.size())\n",
        "prediction = model(test_image.to(device))\n",
        "# access the index of predicted class\n",
        "_, pred = torch.max(prediction,1)\n",
        "print(classes[test_image_label], classes[pred])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wA_wL7SKP9nJ",
        "colab_type": "text"
      },
      "source": [
        "# Evaluate on validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkyM3JE9P8KH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with torch.no_grad():\n",
        "  model.eval()\n",
        "  correct_preds = 0\n",
        "  for image, labels in valloader:\n",
        "    image = image.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    output = model(image)\n",
        "    _, preds = torch.max(output, 1)\n",
        "    # print(f\"actual : {labels.data}, predicted : {preds}\")\n",
        "    correct_preds+=torch.sum(preds == labels.data)\n",
        "    # print(f\"correct_preds : {correct_preds}\")\n",
        "    val_acc = correct_preds.double() / len(val_dataset)\n",
        "  print(f\"val accuracy : {val_acc}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhhMJX7tGRo_",
        "colab_type": "text"
      },
      "source": [
        "# Convnet as fixed feature extractor :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CD7yL_ecWxJs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conv_model = models.resnet18(pretrained=True)\n",
        "\n",
        "for param in conv_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "num_features = conv_model.fc.in_features\n",
        "\n",
        "conv_model.fc = nn.Linear(num_features, 2)\n",
        "\n",
        "conv_model = conv_model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "opt_conv = optim.SGD(conv_model.fc.parameters(), lr=0.001, momentum=0.7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFzinsXyfLA1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_of_epochs = 10\n",
        "\n",
        "for epoch in range(num_of_epochs):\n",
        "  model = conv_model.to(device)\n",
        "  # set to train mode\n",
        "  model.train()\n",
        "  running_loss = 0.0\n",
        "  correct_preds = 0\n",
        "  for images, labels in trainloader:\n",
        "    #access all data\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "    # erase all accumulated gradients\n",
        "    optimizer.zero_grad()\n",
        "    # predictions from model\n",
        "    output = model(images)\n",
        "    # calculate the loss\n",
        "    loss = criterion(output, labels)\n",
        "    #backprop\n",
        "    loss.backward()\n",
        "    #update_weights\n",
        "    optimizer.step()\n",
        "    # predicion on train dataset\n",
        "    _, preds = torch.max(output,1)\n",
        "\n",
        "    running_loss += loss.item()\n",
        "    correct_preds+=torch.sum(preds == labels.data)\n",
        "\n",
        "  print(f\"finished epoch {epoch}\")\n",
        "  print(f\"epoch-{epoch}, loss : {running_loss}, accuracy : {correct_preds.double()/len(train_dataset)}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBbUAm_lfQ0o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with torch.no_grad():\n",
        "  model.eval()\n",
        "  correct_preds = 0\n",
        "  for image, labels in valloader:\n",
        "    image = image.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    output = model(image)\n",
        "    _, preds = torch.max(output, 1)\n",
        "    # print(f\"actual : {labels.data}, predicted : {preds}\")\n",
        "    correct_preds+=torch.sum(preds == labels.data)\n",
        "    # print(f\"correct_preds : {correct_preds}\")\n",
        "    val_acc = correct_preds.double() / len(val_dataset)\n",
        "  print(f\"val accuracy : {val_acc}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZyrYEOjfbPs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}