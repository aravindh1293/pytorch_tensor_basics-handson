{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch-Basics-Part2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMrZdM/oTS+AvnWYBlbP0MJ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixZ0O2VsFuZ-",
        "colab_type": "text"
      },
      "source": [
        "# AutoGrad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTFYnRczF6E7",
        "colab_type": "text"
      },
      "source": [
        "**From Pytorch Docs :**\n",
        "\n",
        "* `autograd` is central to all neural networks pytorch.\n",
        "\n",
        "* The autograd package provides automatic differentiation for all operations on Tensors. It is a define-by-run framework, which means that your backprop is defined by how your code is run, and that every single iteration can be different.\n",
        "\n",
        "* While backpropogation computed ouput in neural network is sent back and gradients computed are updated \n",
        "\n",
        "* primary dtype in pytorch is `torch.Tensor`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kuxfSiHChEH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoEg8LfVVShH",
        "colab_type": "text"
      },
      "source": [
        "`requires_grad` = used when back propogation is required for every tensor.\n",
        "\n",
        "`.backward` = to initiate backpropogation\n",
        "\n",
        "`.grad` = all gradient are accumulateed in this attribute\n",
        "\n",
        "`.grad_fn` = show which operation were done to a tensor. ex: add, multiplication or division.\n",
        "\n",
        "`.detach` = to get new tensor without gradient"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pWx8i7zSGuk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "a4b1a5ee-aaa3-482f-f1f9-9a06d8045291"
      },
      "source": [
        "# lets create a initial tensor to test all the above functions\n",
        "\n",
        "x = torch.ones(3,3, requires_grad=True)\n",
        "print(x)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElpdzPzgYwps",
        "colab_type": "text"
      },
      "source": [
        "Notice that `requires_grad=True` in output, that means we can do any operation to this tensor and do back propogation to that tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWxCxStfYeaF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "64942801-0988-4be5-9990-7e84f80c2edc"
      },
      "source": [
        "# now lets do a addition operation to that base tensor\n",
        "\n",
        "y = x + 2\n",
        "print(y)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[3., 3., 3.],\n",
            "        [3., 3., 3.],\n",
            "        [3., 3., 3.]], grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Fj6QxDrZxQW",
        "colab_type": "text"
      },
      "source": [
        "Now, Notice that `grad_fn` shows that addition operation has been."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyNmWWlOdj-2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "93e7a3c3-1c98-411c-a2f9-c82b8c482154"
      },
      "source": [
        "# lets do more mathematical operation and see `grad_fn` results\n",
        "z = y * y * 3\n",
        "out = z.mean()\n",
        "\n",
        "print(z)\n",
        "print(out)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[27., 27., 27.],\n",
            "        [27., 27., 27.],\n",
            "        [27., 27., 27.]], grad_fn=<MulBackward0>)\n",
            "tensor(27., grad_fn=<MeanBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5DivyxWd3AJ",
        "colab_type": "text"
      },
      "source": [
        "Notice that we get grad_fn as multiplication and mean for both the operations we did on the tensor.\n",
        "\n",
        "now these operations can be back propogated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xV5_5Bf_g091",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "47322cdf-b8d1-4b41-9dd7-00ffdb986d0c"
      },
      "source": [
        "out"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(27., grad_fn=<MeanBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMrWZy9Bg3x2",
        "colab_type": "text"
      },
      "source": [
        "as you can see out contains a single scalar, lets do backprop to this"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHnfFpVgg1El",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "1b48d5d7-db3f-497a-b49a-52a482ac4f4b"
      },
      "source": [
        "# to do backprop we just need to call .backward\n",
        "out.backward()\n",
        "# all the gradients accumulated are stored in a attribute called `.grad`\n",
        "print(x.grad)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[2., 2., 2.],\n",
            "        [2., 2., 2.],\n",
            "        [2., 2., 2.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tw-P0QlnDd2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "df44803d-d52b-44f7-8a90-525460fe0fc4"
      },
      "source": [
        "# lets create a tensor with a requires_grad\n",
        "x = torch.randn(3,3, requires_grad=True)\n",
        "print(x)\n",
        "\n",
        "# suppose we want to get that tensor with no grad_fn, then we can use detach() to get that tensor seperately\n",
        "y = x.detach()\n",
        "print(y)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0672, -0.6789,  0.5611],\n",
            "        [-0.3981,  0.2087,  0.6432],\n",
            "        [-0.3187,  1.7171, -0.8935]], requires_grad=True)\n",
            "tensor([[ 0.0672, -0.6789,  0.5611],\n",
            "        [-0.3981,  0.2087,  0.6432],\n",
            "        [-0.3187,  1.7171, -0.8935]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcCqDicAnrZc",
        "colab_type": "text"
      },
      "source": [
        "Notice that y doesn't have `requires_grad` function. \n",
        "\n",
        "now that tensor can be used seperately for anyother functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRxIS7GAd07L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "3713df6b-b7c8-4d93-a62f-fe1e185d3acf"
      },
      "source": [
        "# suppose we initiate a tensor without grad condition i.e `requires_grad=True`\n",
        "a = torch.randn(2,2)\n",
        "\n",
        "a = ((a-3)/(a-1))\n",
        "print(a.requires_grad)\n",
        "\n",
        "# we can use inplace function in pytorch to replace that to be True like this\n",
        "# in pytorch any method followed by _ is used to replace original value\n",
        "a.requires_grad_(True)\n",
        "print(a.requires_grad)\n",
        "\n",
        "# lets test if grad_fn shows up on any mathematical operations.\n",
        "b = (a*a).sum()\n",
        "print(f\"grad_fn test : {b.grad_fn}\")\n",
        "print(\"yay!! now grad function is added\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n",
            "True\n",
            "grad_fn test : <SumBackward0 object at 0x7f96f85a8208>\n",
            "yay!! now grad function is added\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VG73ngh5fSLX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}